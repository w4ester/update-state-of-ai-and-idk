<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>LLM Hallucinations & ‚ÄúI Don‚Äôt Know‚Äù ‚Äî Oct‚ÄìNov 2025 Snapshot</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root {
      --bg: #0b1020;
      --bg-alt: #151a2b;
      --card: #181f33;
      --accent: #4fd1c5;
      --accent-soft: rgba(79, 209, 197, 0.16);
      --text: #f7fafc;
      --muted: #a0aec0;
      --border: #2d3748;
      --danger: #f56565;
      --shadow-soft: 0 18px 45px rgba(0,0,0,0.55);
      --radius-lg: 18px;
      --radius-pill: 999px;
      --transition-fast: 0.15s ease-out;
    }

    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      background: radial-gradient(circle at top left, #1a2238 0, #050812 50%, #05060b 100%);
      color: var(--text);
      line-height: 1.6;
      padding: 2.5rem 1.25rem 3rem;
      display: flex;
      justify-content: center;
    }

    .page {
      width: 100%;
      max-width: 960px;
    }

    header {
      margin-bottom: 1.75rem;
    }

    .badge {
      display: inline-flex;
      align-items: center;
      gap: 0.4rem;
      padding: 0.15rem 0.85rem;
      border-radius: var(--radius-pill);
      border: 1px solid rgba(79, 209, 197, 0.5);
      background: rgba(15, 23, 42, 0.8);
      color: var(--accent);
      font-size: 0.75rem;
      letter-spacing: 0.06em;
      text-transform: uppercase;
      margin-bottom: 0.75rem;
    }

    .badge-dot {
      width: 7px;
      height: 7px;
      border-radius: 999px;
      background: var(--accent);
      box-shadow: 0 0 12px rgba(79, 209, 197, 0.9);
    }

    h1 {
      font-size: clamp(1.8rem, 3vw, 2.25rem);
      margin-bottom: 0.4rem;
    }

    .subtitle {
      color: var(--muted);
      max-width: 720px;
      font-size: 0.98rem;
    }

    .pill-row {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      margin: 1.1rem 0 1.5rem;
    }

    .pill {
      font-size: 0.78rem;
      padding: 0.28rem 0.8rem;
      border-radius: var(--radius-pill);
      border: 1px solid rgba(148, 163, 184, 0.45);
      color: #cbd5f5;
      background: rgba(15, 23, 42, 0.8);
      white-space: nowrap;
    }

    .layout {
      display: grid;
      grid-template-columns: minmax(0, 2.3fr) minmax(0, 1.3fr);
      gap: 1.5rem;
    }

    @media (max-width: 800px) {
      .layout {
        grid-template-columns: minmax(0, 1fr);
      }
    }

    .card {
      background: radial-gradient(circle at top left, #202a46 0, #15192b 40%, #0f1422 100%);
      border-radius: var(--radius-lg);
      border: 1px solid var(--border);
      padding: 1.1rem 1.25rem 1.25rem;
      box-shadow: var(--shadow-soft);
      position: relative;
      overflow: hidden;
    }

    .card + .card {
      margin-top: 0.85rem;
    }

    .card-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 0.4rem;
      gap: 0.75rem;
    }

    .card-title {
      font-size: 1rem;
      font-weight: 600;
      display: flex;
      align-items: center;
      gap: 0.5rem;
    }

    .tag {
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      color: var(--muted);
      padding: 0.15rem 0.5rem;
      border-radius: var(--radius-pill);
      border: 1px solid rgba(148, 163, 184, 0.3);
    }

    .risk-tag {
      border-color: rgba(245, 101, 101, 0.8);
      color: var(--danger);
      background: rgba(245, 101, 101, 0.08);
    }

    .section-label {
      font-size: 0.75rem;
      color: var(--muted);
      text-transform: uppercase;
      letter-spacing: 0.1em;
      margin-bottom: 0.35rem;
    }

    p {
      font-size: 0.9rem;
      color: #e2e8f0;
      margin-bottom: 0.35rem;
    }

    ul {
      list-style: none;
      margin-left: 0;
    }

    li {
      font-size: 0.88rem;
      margin-bottom: 0.25rem;
      color: #cbd5f5;
      display: flex;
      gap: 0.4rem;
    }

    .bullet {
      width: 0.28rem;
      height: 0.28rem;
      border-radius: 999px;
      background: var(--accent);
      margin-top: 0.45rem;
      flex-shrink: 0;
    }

    .link-list {
      display: flex;
      flex-direction: column;
      gap: 0.55rem;
      margin-top: 0.35rem;
    }

    .link-item {
      font-size: 0.8rem;
      word-break: break-all;
    }

    a {
      color: var(--accent);
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    code {
      font-family: "JetBrains Mono", ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      font-size: 0.78rem;
      background: rgba(15, 23, 42, 0.9);
      padding: 0.12rem 0.35rem;
      border-radius: 0.3rem;
      border: 1px solid rgba(148, 163, 184, 0.35);
      color: #edf2ff;
    }

    .callout {
      background: linear-gradient(120deg, rgba(79, 209, 197, 0.12), transparent);
      border-radius: 12px;
      padding: 0.6rem 0.75rem;
      font-size: 0.82rem;
      color: var(--muted);
      border: 1px solid rgba(79, 209, 197, 0.25);
      margin-top: 0.45rem;
    }

    .callout strong {
      color: #e2e8f0;
    }

    .badge-year {
      position: absolute;
      right: 1.1rem;
      top: 0.9rem;
      font-size: 0.7rem;
      color: var(--muted);
      opacity: 0.8;
    }

    .controls {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      margin-top: 0.7rem;
    }

    button {
      font-family: inherit;
      font-size: 0.8rem;
      border-radius: var(--radius-pill);
      border: 1px solid rgba(148, 163, 184, 0.7);
      background: rgba(15, 23, 42, 0.9);
      color: var(--text);
      padding: 0.38rem 0.9rem;
      cursor: pointer;
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
      transition: background var(--transition-fast), transform var(--transition-fast), box-shadow var(--transition-fast), border-color var(--transition-fast);
    }

    button.primary {
      border-color: rgba(79, 209, 197, 0.9);
      background: radial-gradient(circle at top left, var(--accent-soft) 0, rgba(15, 23, 42, 0.96) 45%, #050812 100%);
    }

    button:hover {
      transform: translateY(-1px);
      box-shadow: 0 10px 25px rgba(0, 0, 0, 0.35);
    }

    button:active {
      transform: translateY(0);
      box-shadow: none;
    }

    .tiny-label {
      font-size: 0.7rem;
      color: var(--muted);
      margin-top: 0.3rem;
    }

    .status {
      font-size: 0.78rem;
      margin-left: 0.4rem;
      color: var(--muted);
    }

    .status.ok {
      color: #9ae6b4;
    }

    .status.error {
      color: var(--danger);
    }

    .hidden {
      display: none;
    }

    .collapsible {
      margin-top: 0.25rem;
    }

    .collapsible-toggle {
      font-size: 0.75rem;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      color: var(--muted);
      cursor: pointer;
      display: inline-flex;
      align-items: center;
      gap: 0.25rem;
      user-select: none;
    }

    .collapsible-toggle-icon {
      transition: transform var(--transition-fast);
      font-size: 0.75rem;
    }

    .collapsible-content {
      margin-top: 0.45rem;
      font-size: 0.86rem;
      color: #e2e8f0;
    }

    .collapsible.open .collapsible-toggle-icon {
      transform: rotate(90deg);
    }

        /* --- Timeline styles --- */
    .timeline-block {
      margin-top: 1.25rem;
    }

    .timeline {
      margin-top: 0.25rem;
    }

    .timeline-track {
      position: relative;
      display: flex;
      gap: 1.5rem;
      padding: 1.4rem 0.4rem 0.3rem;
      overflow-x: auto;
      scrollbar-width: thin;
      scrollbar-color: rgba(148, 163, 184, 0.6) transparent;
    }

    .timeline-track::-webkit-scrollbar {
      height: 6px;
    }

    .timeline-track::-webkit-scrollbar-thumb {
      background: rgba(148, 163, 184, 0.6);
      border-radius: 999px;
    }

    .timeline-track::before {
      content: "";
      position: absolute;
      top: 1.8rem;
      left: 0;
      right: 0;
      height: 2px;
      background: linear-gradient(
        90deg,
        rgba(148, 163, 184, 0.35),
        rgba(79, 209, 197, 0.75)
      );
      pointer-events: none;
    }

    .timeline-item {
      position: relative;
      min-width: 100px;
      max-width: 130px;
      padding-top: 0.1rem;
      font-size: 0.78rem;
      color: #e2e8f0;
    }

    .timeline-date {
      font-size: 0.8rem;
      font-weight: 600;
      color: #e2e8f0;
      margin-bottom: 0.15rem;
    }

    .timeline-dot {
      position: relative;
      width: 11px;
      height: 11px;
      border-radius: 999px;
      border: 2px solid #0b1020;
      margin-bottom: 0.25rem;
      box-shadow: 0 0 0 2px rgba(15, 23, 42, 0.9);
    }

    .timeline-dot.paper {
      background: var(--accent);
    }

    .timeline-dot.model {
      background: #fbbf24;
      box-shadow: 0 0 16px rgba(251, 191, 36, 0.9);
    }

    .timeline-dot::after {
      content: "";
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      width: 4px;
      height: 4px;
      border-radius: 999px;
      background: #0b1020;
      opacity: 0.8;
    }

    .timeline-label {
      font-weight: 500;
      margin-bottom: 0.1rem;
    }

    .timeline-type {
      font-size: 0.72rem;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      color: var(--muted);
    }

    .timeline-legend {
      display: flex;
      flex-wrap: wrap;
      gap: 0.6rem;
      margin-top: 0.7rem;
      font-size: 0.78rem;
      color: var(--muted);
    }

    .timeline-legend-item {
      display: inline-flex;
      align-items: center;
      gap: 0.3rem;
    }

    .timeline-swatch {
      width: 10px;
      height: 10px;
      border-radius: 999px;
    }

    .timeline-swatch.paper {
      background: var(--accent);
    }

    .timeline-swatch.model {
      background: #fbbf24;
    }

  </style>
</head>
<body>
  <div class="page">
    <header>
      <div class="badge">
        <span class="badge-dot"></span>
        LLM Hallucinations ‚Ä¢ Oct‚ÄìNov 2025 Snapshot
      </div>
      <h1>From ‚ÄúWhy LMs Hallucinate‚Äù to Concrete Fixes</h1>
      <p class="subtitle">
        One-page brief for our team: why language models hallucinate (Kalai et&nbsp;al., OpenAI), and what the latest
        Oct‚ÄìNov&nbsp;2025 research is actually doing about it ‚Äî especially around ‚ÄúI don‚Äôt know‚Äù behavior.
      </p>
      <div class="pill-row">
        <span class="pill">Hallucination Mitigation</span>
        <span class="pill">‚ÄúI Don‚Äôt Know‚Äù Training</span>
        <span class="pill">Abstention &amp; New Metrics</span>
        <span class="pill">Detection &amp; Model Teams</span>
      </div>
    </header>

        <!-- VISUAL TIMELINE CARD -->
    <article class="card timeline-block">
      <div class="card-header">
        <div>
          <div class="section-label">0 ¬∑ 2025 timeline</div>
          <div class="card-title">
            Key research & model releases
            <span class="tag">Sep‚ÄìNov 2025</span>
          </div>
        </div>
      </div>
      <p style="font-size:0.85rem;margin-bottom:0.4rem;">
        Rough chronological view of when the main <strong>hallucination / ‚ÄúI don‚Äôt know‚Äù papers</strong>
        and <strong>frontier models</strong> in this overview landed.
      </p>

      <div class="timeline">
        <div class="timeline-track">
          <!-- Sept 4: Why LMs hallucinate -->
          <div class="timeline-item">
            <div class="timeline-date">Sep&nbsp;4</div>
            <div class="timeline-dot paper"></div>
            <div class="timeline-label">Why LMs Hallucinate</div>
            <div class="timeline-type">Paper ¬∑ Kalai et&nbsp;al.</div>
          </div>

          <!-- Sept 29: Claude Sonnet 4.5 -->
          <div class="timeline-item">
            <div class="timeline-date">Sep&nbsp;29</div>
            <div class="timeline-dot model"></div>
            <div class="timeline-label">Claude Sonnet 4.5</div>
            <div class="timeline-type">Model release ¬∑ Anthropic</div>
          </div>

          <!-- Oct 5: Survey & DCS -->
          <div class="timeline-item">
            <div class="timeline-date">Oct&nbsp;5</div>
            <div class="timeline-dot paper"></div>
            <div class="timeline-label">Survey &amp; DCS metric</div>
            <div class="timeline-type">Survey + eval (DCS)</div>
          </div>

          <!-- Oct 13: Internal-state detector -->
          <div class="timeline-item">
            <div class="timeline-date">Oct&nbsp;13</div>
            <div class="timeline-dot paper"></div>
            <div class="timeline-label">Internal-state detector</div>
            <div class="timeline-type">Hallucination detection</div>
          </div>

          <!-- Oct 22‚Äì24: Teams & ABSTAIN -->
          <div class="timeline-item">
            <div class="timeline-date">Oct&nbsp;22‚Äì24</div>
            <div class="timeline-dot paper"></div>
            <div class="timeline-label">LLM teams &amp; ABSTAIN token</div>
            <div class="timeline-type">Ensembles + abstention</div>
          </div>

          <!-- Nov 12: ChatGPT / GPT-5.1 -->
          <div class="timeline-item">
            <div class="timeline-date">Nov&nbsp;12</div>
            <div class="timeline-dot model"></div>
            <div class="timeline-label">ChatGPT&nbsp;5.1 / GPT-5.1</div>
            <div class="timeline-type">Model upgrade ¬∑ OpenAI</div>
          </div>

          <!-- Mid-Nov: Reinforced Hesitation -->
          <div class="timeline-item">
            <div class="timeline-date">Nov&nbsp;2025</div>
            <div class="timeline-dot paper"></div>
            <div class="timeline-label">Reinforced Hesitation</div>
            <div class="timeline-type">IDK-focused training</div>
          </div>

          <!-- Nov 17: Weight-sparse circuits -->
          <div class="timeline-item">
            <div class="timeline-date">Nov&nbsp;17</div>
            <div class="timeline-dot paper"></div>
            <div class="timeline-label">Weight-sparse circuits</div>
            <div class="timeline-type">Mechanistic interp.</div>
          </div>

          <!-- Nov 18: Gemini 3 -->
          <div class="timeline-item">
            <div class="timeline-date">Nov&nbsp;18</div>
            <div class="timeline-dot model"></div>
            <div class="timeline-label">Gemini&nbsp;3</div>
            <div class="timeline-type">Model release ¬∑ Google</div>
          </div>
        </div>

        <div class="timeline-legend">
          <div class="timeline-legend-item">
            <span class="timeline-swatch paper"></span>
            <span>Papers / research</span>
          </div>
          <div class="timeline-legend-item">
            <span class="timeline-swatch model"></span>
            <span>Frontier model releases</span>
          </div>
        </div>
      </div>
    </article>


    <main class="layout">
      <!-- LEFT COLUMN: STORY & TAKEAWAYS -->
      <section>
        <!-- Why hallucinate -->
        <article class="card">
          <div class="badge-year">Sep 2025</div>
          <div class="card-header">
            <div>
              <div class="section-label">1 ¬∑ Why LMs hallucinate</div>
              <div class="card-title">
                Why Language Models Hallucinate
                <span class="tag">Kalai et&nbsp;al., OpenAI</span>
              </div>
            </div>
          </div>
          <p>
            The core message from <strong>‚ÄúWhy Language Models Hallucinate‚Äù</strong> and the OpenAI blog:
            hallucinations are not a weird bug ‚Äî they‚Äôre a <strong>statistically expected error mode</strong>
            under current training + evaluation.
          </p>
          <ul>
            <li><span class="bullet"></span><span>
              <strong>Pretraining:</strong> even with perfect data, next-token prediction plus limited capacity
              guarantees some plausible-but-wrong strings in the model‚Äôs distribution.
            </span></li>
            <li><span class="bullet"></span><span>
              <strong>Post-training + scoreboards:</strong> benchmarks give the same 0 points for a wrong answer and
              for ‚ÄúI don‚Äôt know,‚Äù so models are literally <em>rewarded</em> for guessing instead of abstaining.
            </span></li>
            <li><span class="bullet"></span><span>
              Result: models become excellent <strong>test-takers</strong> but poor at expressing uncertainty ‚Äî hence
              confident hallucinations.
            </span></li>
          </ul>
          <div class="callout">
            <strong>Key idea:</strong> Fixing hallucinations is partly a <em>socio-technical</em> problem:
            change how we <strong>score</strong> evaluations so that wrong answers are penalized more than honest
            ‚ÄúI don‚Äôt know.‚Äù
          </div>
        </article>

        <!-- New fixes -->
        <article class="card">
          <div class="badge-year">Oct‚ÄìNov 2025</div>
          <div class="card-header">
            <div>
              <div class="section-label">2 ¬∑ What‚Äôs new in Oct‚ÄìNov 2025</div>
              <div class="card-title">
                Concrete fixes for hallucinations &amp; IDK
                <span class="tag risk-tag">Mitigations</span>
              </div>
            </div>
          </div>
          <p><strong>2.1 Training-time: teach models to hesitate</strong></p>
          <ul>
            <li><span class="bullet"></span><span>
              <strong>Reinforced Hesitation (RH)</strong> ‚Äî <em>Honesty over Accuracy: Trustworthy LMs through Reinforced Hesitation</em><br />
              Uses a <code>+1 / 0 / ‚àíŒª</code> reward: correct = +1, ‚ÄúI don‚Äôt know‚Äù = 0, wrong = ‚àíŒª. For logic/QA tasks,
              this creates models that <strong>answer when confident and abstain when risky</strong>, dramatically
              shrinking error rates at the cost of more IDK‚Äôs.
            </span></li>
            <li><span class="bullet"></span><span>
              <strong>ABSTAIN token</strong> ‚Äî <em>All We Also Need Is ABSTAIN: Eliminating Hallucinations via a Single Token</em><br />
              Adds a dedicated <code>&lt;ABSTAIN&gt;</code> token and trains on corrupted prompts. On unseen facts,
              the model learns to output <code>&lt;ABSTAIN&gt;</code> instead of fabricating ‚Äî turning many
              hallucinations into explicit IDK responses.
            </span></li>
          </ul>

          <p style="margin-top:0.6rem;"><strong>2.2 Evaluation: new scoring rules</strong></p>
          <ul>
            <li><span class="bullet"></span><span>
              <strong>Distributional Correctness Score (DCS)</strong> ‚Äî <em>Measuring LLM Hallucinations Through Distributional Correctness</em><br />
              Scores the <em>full probability distribution</em>, rewarding probability mass on the correct answer
              or IDK and penalizing mass on wrong answers. Encourages uncertainty to flow into ‚ÄúI don‚Äôt know,‚Äù
              not into polished wrong statements.
            </span></li>
          </ul>

          <p style="margin-top:0.6rem;"><strong>2.3 Detection &amp; ensembles: catching hallucinations before users see them</strong></p>
          <ul>
            <li><span class="bullet"></span><span>
              <strong>Internal-state + reasoning consistency</strong> ‚Äî <em>Hallucination Detection via Internal States and Structured Reasoning Consistency</em><br />
              Combines internal activations with chain-of-thought consistency checks via cross-attention, yielding
              state-of-the-art hallucination detection across fact-heavy and reasoning-heavy tasks.
            </span></li>
            <li><span class="bullet"></span><span>
              <strong>Model teams / consortium consistency</strong> ‚Äî <em>Teaming LLMs to Detect and Mitigate Hallucinations</em><br />
              Uses multiple diverse LLMs and agreement-based voting. Large disagreement is treated as a signal to
              abstain, escalate, or fall back to retrieval, cutting hallucinations versus single-model setups.
            </span></li>
            <li><span class="bullet"></span><span>
              <strong>Surveys &amp; systematization</strong> ‚Äî <em>A Comprehensive Survey of Hallucination in LLMs: Causes, Detection, and Mitigation</em><br />
              Pulls together causes, detector families, and mitigation strategies, reinforcing that effective
              solutions are a <strong>stack</strong>: training, metrics, detectors, and product policy.
            </span></li>
          </ul>

          <p style="margin-top:0.6rem;"><strong>2.4 Mechanistic interpretability: making circuits legible</strong></p>
          <ul>
            <li><span class="bullet"></span><span>
              <strong>Weight-sparse transformers have interpretable circuits (Gao et&nbsp;al., 2025)</strong> introduce weight-sparse training
              and pruning methods that yield small, human-understandable circuits for simple tasks. Although not a
              hallucination paper, it gives us tools to:
              <br /><br />
              &ndash; See how specific behaviors (e.g., regex-like pattern detection, bracket counting) are implemented;<br />
              &ndash; Study capability‚Äìinterpretability trade-offs;<br />
              &ndash; Eventually build circuit-level detectors or IDK-gates that distinguish well-understood circuits from messy ones.
              <br /><br />
              So: it doesn‚Äôt directly change rewards or teach ‚ÄúI don‚Äôt know,‚Äù but it deepens the ‚Äúhow does the model
              actually compute?‚Äù layer that the other hallucination/IDK work can build on.
            </span></li>
          </ul>

          <div class="collapsible" data-collapsible>
            <div class="collapsible-toggle">
              <span class="collapsible-toggle-icon">‚ñ∂</span>
              More context from Science / media
            </div>
            <div class="collapsible-content hidden">
              <p>
                A recent <em>Science</em> news piece (<strong>‚ÄúAI hallucinates because it‚Äôs trained to fake it till it
                makes it‚Äù</strong>) distills this for non-experts: current models are optimized like students on exams
                that punish ‚ÄúI don‚Äôt know,‚Äù so they confidently make things up. Proposed remedy: teach models to
                admit uncertainty and change scoreboards to reward that honesty.
              </p>
            </div>
          </div>
        </article>

        <!-- What our lab can do -->
        <article class="card">
          <div class="card-header">
            <div>
              <div class="section-label">3 ¬∑ Suggested actions for our lab</div>
              <div class="card-title">
                How to operationalize this work
              </div>
            </div>
          </div>
          <ul>
            <li><span class="bullet"></span><span>
              <strong>Adopt ternary rewards or abstention-aware scoring</strong> in our evaluations:
              separate <em>correct / wrong / abstain</em>. Penalize wrong answers more than IDK,
              especially in high-risk domains.
            </span></li>
            <li><span class="bullet"></span><span>
              <strong>Add an explicit abstain primitive</strong> (token or action) and train with
              unanswerable/corrupted prompts so models have a native way to say ‚ÄúI don‚Äôt know.‚Äù
            </span></li>
            <li><span class="bullet"></span><span>
              <strong>Front models with detectors and/or model teams</strong>: use internal-state +
              CoT-based detectors and, where needed, ensembles of diverse LLMs to gate outputs
              and trigger retrieval or escalation when risk is high.
            </span></li>
            <li><span class="bullet"></span><span>
              <strong>Align product policy with this behavior</strong>: document that, for certain
              workloads, ‚Äúfrequent but honest IDK‚Äù is preferable to fewer but overconfident answers.
            </span></li>
          </ul>
          <div class="callout">
            <strong>Bottom line:</strong> hallucinations won‚Äôt fully disappear, but we can
            <em>re-route</em> many of them into refusals/IDKs and make the remaining ones easier
            to detect, measure, and mitigate.
          </div>
        </article>
      </section>

      <!-- RIGHT COLUMN: LINKS & UTILITIES -->
      <aside>
        <article class="card">
          <div class="card-header">
            <div>
              <div class="section-label">4 ¬∑ Core resources</div>
              <div class="card-title">
                Papers &amp; articles (Oct‚ÄìNov 2025)
              </div>
            </div>
          </div>

          <p style="margin-bottom:0.3rem;">Copy-and-paste-friendly links for deeper reading:</p>
          <div id="linkContainer" class="link-list">
            <!-- Each item has data-link for JS "copy all" -->
            <div class="link-item" data-link="https://arxiv.org/abs/2509.04664">
              1. Why Language Models Hallucinate ‚Äî Kalai et&nbsp;al., 2025<br />
              <code>https://arxiv.org/abs/2509.04664</code>
            </div>
            <div class="link-item" data-link="https://openai.com/index/why-language-models-hallucinate/">
              2. OpenAI blog: Why language models hallucinate<br />
              <code>https://openai.com/index/why-language-models-hallucinate/</code>
            </div>
            <div class="link-item" data-link="https://arxiv.org/abs/2511.11500">
              3. Honesty over Accuracy: Trustworthy LMs through Reinforced Hesitation<br />
              <code>https://arxiv.org/abs/2511.11500</code>
            </div>
            <div class="link-item" data-link="https://www.preprints.org/manuscript/202510.1827">
              4. All We Also Need Is ABSTAIN: Eliminating Hallucinations via a Single Token<br />
              <code>https://www.preprints.org/manuscript/202510.1827</code>
            </div>
            <div class="link-item" data-link="https://arxiv.org/abs/2510.04302">
              5. Measuring LLM Hallucinations Through Distributional Correctness<br />
              <code>https://arxiv.org/abs/2510.04302</code>
            </div>
            <div class="link-item" data-link="https://arxiv.org/abs/2510.11529">
              6. Hallucination Detection via Internal States and Structured Reasoning Consistency in LLMs<br />
              <code>https://arxiv.org/abs/2510.11529</code>
            </div>
            <div class="link-item" data-link="https://arxiv.org/abs/2510.19507">
              7. Teaming LLMs to Detect and Mitigate Hallucinations<br />
              <code>https://arxiv.org/abs/2510.19507</code>
            </div>
            <div class="link-item" data-link="https://arxiv.org/abs/2510.06265">
              8. A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation<br />
              <code>https://arxiv.org/abs/2510.06265</code>
            </div>
            <div class="link-item" data-link="https://www.science.org/content/article/ai-hallucinates-because-it-s-trained-fake-answers-it-doesn-t-know">
              9. Science news: AI hallucinates because it‚Äôs trained to fake answers it doesn‚Äôt know<br />
              <code>https://www.science.org/content/article/ai-hallucinates-because-it-s-trained-fake-answers-it-doesn-t-know</code>
            </div>
            <div class="link-item" data-link="https://arxiv.org/pdf/2511.13653">
              10. Weight-sparse transformers have interpretable circuits ‚Äî Gao et&nbsp;al., 2025<br />
              <code>https://arxiv.org/pdf/2511.13653</code>
            </div>

            <div class="link-item" data-link="https://arxiv.org/pdf/2502.15657">
              11. Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?<br />
              <code>https://arxiv.org/pdf/2502.15657</code>
            </div>
          </div>

          <div class="controls">
            <button class="primary" id="copyLinksBtn" type="button">
              üìã Copy all links
            </button>
            <button id="toggleLinksBtn" type="button">
              üëÅ Toggle raw URLs
            </button>
          </div>
          <div id="copyStatus" class="tiny-label"></div>
        </article>

        <article class="card">
          <div class="card-header">
            <div>
              <div class="section-label">5 ¬∑ How to use this page</div>
              <div class="card-title">For GitHub Pages &amp; internal sharing</div>
            </div>
          </div>
          <ul>
            <li><span class="bullet"></span><span>
              Drop this file in a GitHub repo as <code>index.html</code> and enable GitHub Pages.
            </span></li>
            <li><span class="bullet"></span><span>
              Share the URL internally as a quick primer on hallucination research &amp; mitigations.
            </span></li>
            <li><span class="bullet"></span><span>
              Use the <strong>Copy all links</strong> button to paste citations into docs or slide decks.
            </span></li>
          </ul>
        </article>
      </aside>
    </main>
  </div>

  <script>
    // Copy all resource links to clipboard
    (function () {
      const copyBtn = document.getElementById("copyLinksBtn");
      const toggleBtn = document.getElementById("toggleLinksBtn");
      const statusEl = document.getElementById("copyStatus");
      const linkContainer = document.getElementById("linkContainer");

      function setStatus(message, kind) {
        if (!statusEl) return;
        statusEl.textContent = message || "";
        statusEl.classList.remove("ok", "error");
        if (kind) statusEl.classList.add(kind);
      }

      if (copyBtn) {
        copyBtn.addEventListener("click", async () => {
          try {
            const items = Array.from(
              linkContainer.querySelectorAll("[data-link]")
            );
            const lines = items.map((el, i) => {
              const url = el.getAttribute("data-link") || "";
              return `${i + 1}. ${url}`;
            });
            const text = lines.join("\n");
            await navigator.clipboard.writeText(text);
            setStatus("Links copied to clipboard.", "ok");
          } catch (err) {
            console.error(err);
            setStatus("Could not access clipboard in this browser.", "error");
          }
        });
      }

      if (toggleBtn) {
        toggleBtn.addEventListener("click", () => {
          const codes = linkContainer.querySelectorAll("code");
          codes.forEach((code) => {
            code.classList.toggle("hidden");
          });
        });
      }

      // Simple collapsible sections
      document.querySelectorAll("[data-collapsible]").forEach((wrap) => {
        const toggle = wrap.querySelector(".collapsible-toggle");
        const content = wrap.querySelector(".collapsible-content");
        if (!toggle || !content) return;

        toggle.addEventListener("click", () => {
          const isHidden = content.classList.contains("hidden");
          content.classList.toggle("hidden", !isHidden);
          wrap.classList.toggle("open", isHidden);
        });
      });
    })();
  </script>
</body>
</html>
